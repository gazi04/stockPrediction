{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a99434-e99c-4f6e-baa9-fb3f1c5f6ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d285d7da-f03f-4929-9c7b-690a2806e4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NLTK_INITIALIZED = False\n",
    "\n",
    "def initialize_nltk():\n",
    "    global NLTK_INITIALIZED\n",
    "    import nltk\n",
    "    \n",
    "    if not NLTK_INITIALIZED:\n",
    "        NLTK_DATA_PATH = '/tmp/nltk_data'\n",
    "        os.makedirs(NLTK_DATA_PATH, exist_ok=True)\n",
    "        \n",
    "        nltk.data.path.append(NLTK_DATA_PATH)\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download('punkt_tab', download_dir=NLTK_DATA_PATH, quiet=True)\n",
    "                nltk.download('punkt', download_dir=NLTK_DATA_PATH, quiet=True)\n",
    "                nltk.download('vader_lexicon', download_dir=NLTK_DATA_PATH, quiet=True)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        NLTK_INITIALIZED = True\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def filter_relevant_sentences(content_series: pd.Series, company_name_series: pd.Series) -> pd.Series:\n",
    "    initialize_nltk()\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "    output_rows = []\n",
    "    \n",
    "    for article, company in zip(content_series, company_name_series):\n",
    "        if not article or not isinstance(article, str) or not company:\n",
    "            output_rows.append([])\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            sentences = sent_tokenize(article)\n",
    "        except Exception:\n",
    "            output_rows.append([])\n",
    "            continue\n",
    "            \n",
    "        target_company = str(company).lower().strip()\n",
    "        \n",
    "        relevant = [\n",
    "            sent for sent in sentences \n",
    "            if target_company in sent.lower()\n",
    "        ]\n",
    "        \n",
    "        output_rows.append(relevant)\n",
    "        \n",
    "    return pd.Series(output_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3444132-87e4-4d4e-aed7-e95c1fd89e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize NLTK VADER (Small download, usually allowed or easy to bundle)\n",
    "nltk.download('vader_lexicon', download_dir='/tmp/nltk_data')\n",
    "nltk.data.path.append('/tmp/nltk_data')\n",
    "\n",
    "@pandas_udf('double')\n",
    "def vader_sentiment(text_series: pd.Series) -> pd.Series:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = []\n",
    "\n",
    "    for text in text_series:\n",
    "        if not text:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "        # 'compound' score ranges from -1 (Neg) to +1 (Pos)\n",
    "        scores.append(analyzer.polarity_scores(text)['compound'])\n",
    "\n",
    "    return pd.Series(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb40117-5797-4f3d-9c6c-9b087774fe24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "companies_df = spark.read.table(\"stock_prediction.default.companies\")\n",
    "\n",
    "# Removes punctuation\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"name\"), '[^a-zA-Z0-9\\\\s]', '')\n",
    ")\n",
    "\n",
    "regex_pattern = r\"\\b(Inc|Corporation|Incorporated|Corp|Ltd|Co)\\b\"\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"clean_name\"), regex_pattern, '')\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\",\n",
    "    sf.lower(sf.col(\"clean_name\"))\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumnRenamed(\"id\", \"company_id\")\n",
    "\n",
    "display(companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611f0e98-1a51-4ad4-b0c3-fc321c9b1931",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764677042055}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "articles_df = spark.read.table(\"stock_prediction.default.articles\")\n",
    "\n",
    "# Cross join articles with companies to check for company mentions in article content\n",
    "joined_df = articles_df.crossJoin(companies_df)\n",
    "\n",
    "# Filter where clean_name is contained in content_cleaned (case-insensitive)\n",
    "joined_df = joined_df.filter(\n",
    "    expr(\"lower(content_cleaned) LIKE concat('%', clean_name, '%')\")\n",
    ")\n",
    "\n",
    "joined_df = joined_df.select(\"id\", \"company_id\", \"name\", \"title\", \"clean_name\", \"content_cleaned\", \"published_at\")\n",
    "\n",
    "articles_with_relevant_sentences = joined_df.withColumn(\n",
    "    \"filtered_sentences\", \n",
    "    filter_relevant_sentences(col(\"content_cleaned\"), col(\"clean_name\"))\n",
    ")\n",
    "\n",
    "display(articles_with_relevant_sentences.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81f2266-8a60-45a4-966f-ef6cc08a48c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, when, avg, expr\n",
    "\n",
    "final_df = articles_with_relevant_sentences.select(\n",
    "    \"id\", \n",
    "    \"company_id\", \n",
    "    \"name\",\n",
    "    \"title\",\n",
    "    \"published_at\",\n",
    "    explode(\"filtered_sentences\").alias(\"sentence\")\n",
    ")\n",
    "\n",
    "final_df = final_df.withColumn(\n",
    "    \"numeric_score\", \n",
    "    vader_sentiment(col(\"sentence\"))\n",
    ")\n",
    "\n",
    "final_df = final_df.groupBy(\"id\", \"company_id\") \\\n",
    "    .agg(\n",
    "        expr(\"first(title)\").alias(\"article_title\"),\n",
    "        expr(\"first(name)\").alias(\"name\"),\n",
    "        expr(\"first(published_at)\").alias(\"published_date\"),\n",
    "        avg(\"numeric_score\").alias(\"score\")\n",
    "    )\n",
    "\n",
    "final_df = final_df.select(col(\"id\").alias(\"article_id\"), \"company_id\", \"score\")\n",
    "final_df.createOrReplaceTempView(\"articles_sentiment_scored_view\")\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c0679e-ff4f-4be3-a4c0-6a5cb43e04a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "MERGE INTO stock_prediction.default.article_sentiment_scored AS target\n",
    "USING articles_sentiment_scored_view AS source\n",
    "ON target.article_id = source.article_id AND target.company_id = source.company_id\n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET score = source.score\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (`article_id`, `company_id`, `score`)\n",
    "  VALUES (source.article_id, source.company_id, source.score)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5754945395948272,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sentiment_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
