{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c41ddd5-3cf8-4313-96e1-b017f4bdfb74",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764176655258}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "companies_df = spark.read.table(\"stock_prediction.default.companies\")\n",
    "\n",
    "# Removes punctuation\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"name\"), '[^a-zA-Z0-9\\\\s]', '')\n",
    ")\n",
    "\n",
    "regex_pattern = r\"\\b(Inc|Corporation|Incorporated|Corp|Ltd|Co)\\b\"\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"clean_name\"), regex_pattern, '')\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\",\n",
    "    sf.lower(sf.col(\"clean_name\"))\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumnRenamed(\"id\", \"company_id\")\n",
    "\n",
    "display(companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed6bb4e-c220-421b-8a39-c0d8bd42ceda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "articles_df = spark.read.table(\"stock_prediction.default.articles\")\n",
    "\n",
    "# Cross join articles with companies to check for company mentions in article content\n",
    "joined_df = articles_df.crossJoin(companies_df)\n",
    "\n",
    "# Filter where clean_name is contained in content_cleaned (case-insensitive)\n",
    "filtered_df = joined_df.filter(\n",
    "    expr(\"lower(content_cleaned) LIKE concat('%', clean_name, '%')\")\n",
    ")\n",
    "\n",
    "# Group by company and collect articles mentioning each company\n",
    "grouped_df = filtered_df.groupBy(\"clean_name\").agg(\n",
    "    sf.collect_list(\"id\").alias(\"article_ids\"),\n",
    "    sf.collect_list(\"title\").alias(\"article_titles\"),\n",
    "    # sf.collect_list(\"content\").alias(\"article_contents\")\n",
    ")\n",
    "\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dae098-1bd8-4e81-8dcb-063cee2f158a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch pandas nltk\n",
    "%restart_python\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68f4f9c-5c38-4a95-bac7-cbcec3d38426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Setup the Model on the Driver (to ensure it downloads correctly)\n",
    "# We use a pipeline for simplicity. It handles tokenization automatically.\n",
    "# device=0 uses the GPU if available, -1 uses CPU.\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=\"ProsusAI/finbert\", \n",
    "    return_all_scores=True,\n",
    "    device=-1 # Set to 0 if your Databricks cluster has GPUs\n",
    ")\n",
    "\n",
    "# 2. Define the Pandas UDF\n",
    "# This function receives a SERIES of text (a batch), not just one string.\n",
    "@pandas_udf('double')\n",
    "def calculate_sentiment_score(text_series: pd.Series) -> pd.Series:\n",
    "    # Reload the pipeline inside the worker to prevent serialization issues\n",
    "    # Note: In a production job, you might load this globally or use a broadcast variable\n",
    "    pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\", return_all_scores=True, device=-1)\n",
    "    \n",
    "    # Run inference on the whole batch at once (much faster)\n",
    "    # Truncation=True ensures long articles don't crash the model\n",
    "    results = pipe(text_series.tolist(), truncation=True, max_length=512)\n",
    "    \n",
    "    # 3. Process Results\n",
    "    # FinBERT returns 3 scores: Positive, Negative, Neutral.\n",
    "    # We want a single \"Compound\" score: Positive - Negative.\n",
    "    final_scores = []\n",
    "    for res in results:\n",
    "        # res looks like: [{'label': 'positive', 'score': 0.9}, {'label': 'negative', 'score': 0.01}, ...]\n",
    "        pos = next(item['score'] for item in res if item['label'] == 'positive')\n",
    "        neg = next(item['score'] for item in res if item['label'] == 'negative')\n",
    "        \n",
    "        # Calculate compound score (-1 to 1)\n",
    "        final_scores.append(pos - neg)\n",
    "        \n",
    "    return pd.Series(final_scores)\n",
    "\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def split_and_filter_sentences(content: str, company_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Splits content into sentences and filters for those mentioning the company.\n",
    "    \"\"\"\n",
    "    if not content:\n",
    "        return []\n",
    "        \n",
    "    sentences = sent_tokenize(content)\n",
    "    \n",
    "    lower_company = company_name.lower()\n",
    "    \n",
    "    filtered_sentences = [\n",
    "        s for s in sentences\n",
    "        if lower_company in s.lower()\n",
    "    ]\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "197db098-39f4-4a70-8b00-009b311223f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018a2863-1fd4-4374-b276-4dc99cb8c3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Apply the UDF\n",
    "# This adds a new column 'sentiment_score' to your DataFrame\n",
    "analyzed_df = grouped_df.withColumn(\n",
    "    \"sentiment_score\", \n",
    "    calculate_sentiment_score(grouped_df.content)\n",
    ")\n",
    "\n",
    "# 3. Save the results (e.g., to a new table or overwrite)\n",
    "analyzed_df.write.mode(\"overwrite\").saveAsTable(\"articles_with_sentiment\")\n",
    "\n",
    "# Verify\n",
    "display(analyzed_df.select(\"title\", \"sentiment_score\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6277742932479671,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "calculate_sentiment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
