{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a99434-e99c-4f6e-baa9-fb3f1c5f6ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch pandas nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf701475-ed86-4584-8dbf-b47a8682aa94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\", return_all_scores=True, device=-1)\n",
    "\n",
    "result = sentiment_pipe(\"With the new iPhone launch Apple took a very bad launch that caused a loss.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae2adcb-370b-4ea3-8562-8cc014bc5e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import os # Import os for path manipulation\n",
    "\n",
    "# 1. Define the writable path (Use this same path for both download and search)\n",
    "NLTK_DATA_PATH = '/tmp/nltk_data'\n",
    "\n",
    "# 2. Add the custom path to NLTK's search list (CRITICAL STEP)\n",
    "# This ensures that when NLTK looks for 'punkt_tab', it checks /tmp/nltk_data.\n",
    "nltk.data.path.append(NLTK_DATA_PATH)\n",
    "\n",
    "# 3. Download the resource to the specified path\n",
    "# Note: You should be downloading 'punkt', not 'punkt_tab'. 'punkt_tab' is an internal error \n",
    "# NLTK throws when it can't find 'punkt'. Let's stick to 'punkt'.\n",
    "nltk.download('punkt', download_dir=NLTK_DATA_PATH, quiet=True) \n",
    "\n",
    "# Ensure the download directory exists (though the download function usually handles this)\n",
    "os.makedirs(NLTK_DATA_PATH, exist_ok=True)\n",
    "\n",
    "sentence_tokenization_pipe = sent_tokenize\n",
    "text = \"\"\"\n",
    "Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n",
    "\"\"\"\n",
    "# This now executes successfully because NLTK knows where to find the 'punkt' data.\n",
    "result = sentence_tokenization_pipe(text) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ad7b36-f7ab-47dc-929e-e6f68b6695c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "export HF_HOME='/tmp/huggingface_cache'\n",
    "echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31b6e46-3a62-48bd-a936-5a1fbf01c7b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SENTIMENT_PIPE, SENTENCE_TOKENIZATION_PIPE = None, None\n",
    "\n",
    "def initialize_models():\n",
    "    \"\"\"Initializes the heavy Hugging Face models once per worker process.\"\"\"\n",
    "    import os\n",
    "    global SENTIMENT_PIPE, SENTENCE_TOKENIZATION_PIPE\n",
    "    \n",
    "\n",
    "    if SENTIMENT_PIPE is None:\n",
    "        from transformers import pipeline   \n",
    "        CACHE_DIR = '/tmp/huggingface_cache'\n",
    "        os.environ['HF_HOME'] = CACHE_DIR\n",
    "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "        SENTIMENT_PIPE = pipeline(\n",
    "            \"text-classification\", \n",
    "            model=\"ProsusAI/finbert\", \n",
    "            return_all_scores=True, \n",
    "            device=-1,\n",
    "            model_kwargs={\"cache_dir\": CACHE_DIR}\n",
    "        )\n",
    "        \n",
    "    if SENTENCE_TOKENIZATION_PIPE is None:\n",
    "        import nltk\n",
    "        NLTK_DATA_PATH = '/tmp/nltk_data'\n",
    "        nltk.data.path.append(NLTK_DATA_PATH)\n",
    "        nltk.download('punkt', download_dir=NLTK_DATA_PATH, quiet=True) \n",
    "\n",
    "        os.makedirs(NLTK_DATA_PATH, exist_ok=True)\n",
    "        SENTENCE_TOKENIZATION_PIPE = sent_tokenize\n",
    "\n",
    "@pandas_udf('double')\n",
    "def calculate_contextual_sentiment(sentence_lists: pd.Series) -> pd.Series:\n",
    "    initialize_models()\n",
    "    \n",
    "    final_scores = []\n",
    "    \n",
    "    for s_list in sentence_lists:\n",
    "        if not s_list or len(s_list) == 0:\n",
    "            final_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            results = SENTIMENT_PIPE(list(s_list), truncation=True, max_length=512)\n",
    "        except Exception:\n",
    "            final_scores.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        article_scores = []\n",
    "        for res in results:\n",
    "            # res format: [{'label': 'positive', 'score': 0.9}, ...]\n",
    "            pos = next((x['score'] for x in res if x['label'] == 'positive'), 0.0)\n",
    "            neg = next((x['score'] for x in res if x['label'] == 'negative'), 0.0)\n",
    "            article_scores.append(pos - neg)\n",
    "            \n",
    "        if article_scores:\n",
    "            final_scores.append(float(np.mean(article_scores)))\n",
    "        else:\n",
    "            final_scores.append(0.0)\n",
    "            \n",
    "    return pd.Series(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d285d7da-f03f-4929-9c7b-690a2806e4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NLTK_INITIALIZED = False\n",
    "\n",
    "def initialize_nltk():\n",
    "    \"\"\"Sets up NLTK on the worker node.\"\"\"\n",
    "    global NLTK_INITIALIZED\n",
    "    import nltk\n",
    "    \n",
    "    if not NLTK_INITIALIZED:\n",
    "        NLTK_DATA_PATH = '/tmp/nltk_data'\n",
    "        os.makedirs(NLTK_DATA_PATH, exist_ok=True)\n",
    "        \n",
    "        nltk.data.path.append(NLTK_DATA_PATH)\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt', download_dir=NLTK_DATA_PATH, quiet=True)\n",
    "            try:\n",
    "                nltk.download('punkt_tab', download_dir=NLTK_DATA_PATH, quiet=True)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        NLTK_INITIALIZED = True\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def filter_relevant_sentences(content_series: pd.Series, company_name_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Splits articles into sentences and returns ONLY the sentences \n",
    "    that contain the specific company name.\n",
    "    \"\"\"\n",
    "    \n",
    "    initialize_nltk()\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "    output_rows = []\n",
    "    \n",
    "    for article, company in zip(content_series, company_name_series):\n",
    "        if not article or not isinstance(article, str) or not company:\n",
    "            output_rows.append([])\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            sentences = sent_tokenize(article)\n",
    "        except Exception:\n",
    "            output_rows.append([])\n",
    "            continue\n",
    "            \n",
    "        target_company = str(company).lower().strip()\n",
    "        \n",
    "        relevant = [\n",
    "            sent for sent in sentences \n",
    "            if target_company in sent.lower()\n",
    "        ]\n",
    "        \n",
    "        output_rows.append(relevant)\n",
    "        \n",
    "    return pd.Series(output_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb40117-5797-4f3d-9c6c-9b087774fe24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "companies_df = spark.read.table(\"stock_prediction.default.companies\")\n",
    "\n",
    "# Removes punctuation\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"name\"), '[^a-zA-Z0-9\\\\s]', '')\n",
    ")\n",
    "\n",
    "regex_pattern = r\"\\b(Inc|Corporation|Incorporated|Corp|Ltd|Co)\\b\"\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"clean_name\"), regex_pattern, '')\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\",\n",
    "    sf.lower(sf.col(\"clean_name\"))\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumnRenamed(\"id\", \"company_id\")\n",
    "\n",
    "display(companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611f0e98-1a51-4ad4-b0c3-fc321c9b1931",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764677042055}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "articles_df = spark.read.table(\"stock_prediction.default.articles\")\n",
    "\n",
    "# Cross join articles with companies to check for company mentions in article content\n",
    "joined_df = articles_df.crossJoin(companies_df)\n",
    "\n",
    "# Filter where clean_name is contained in content_cleaned (case-insensitive)\n",
    "filtered_df = joined_df.filter(\n",
    "    expr(\"lower(content_cleaned) LIKE concat('%', clean_name, '%')\")\n",
    ")\n",
    "\n",
    "final_df = filtered_df.select(\"id\", \"company_id\", \"name\", \"title\", \"clean_name\", \"content_cleaned\", \"published_at\")\n",
    "uber = final_df.where(sf.col(\"clean_name\") == \"uber technologies \")\n",
    "\n",
    "uber = uber.withColumn(\n",
    "    \"filtered_sentences\", \n",
    "    filter_relevant_sentences(col(\"content_cleaned\"), col(\"clean_name\"))\n",
    ")\n",
    "\n",
    "display(uber.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14be1b92-5052-4a1f-81fd-b734c083284f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = uber.select('id', 'company_id', 'filtered_sentences')\n",
    "\n",
    "x = x.withColumn(\n",
    "    \"sentiment_score\", \n",
    "    calculate_contextual_sentiment(col(\"filtered_sentences\"))\n",
    ")\n",
    "display(x.select(\"sentiment_score\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f361fde0-4530-4078-b968-2cb8921c0dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New Cell 4 (Filter Logic without Grouping)\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "articles_df = spark.read.table(\"stock_prediction.default.articles\")\n",
    "\n",
    "# Cross join to get potential matches\n",
    "joined_df = articles_df.crossJoin(companies_df)\n",
    "\n",
    "# Filter down to only rows where the company is actually mentioned\n",
    "# We keep the rows INDIVIDUAL. We do NOT group them.\n",
    "filtered_df = joined_df.filter(\n",
    "    expr(\"lower(content_cleaned) LIKE concat('%', clean_name, '%')\")\n",
    ")\n",
    "\n",
    "# Optional: If you want to test with just Uber first\n",
    "uber_df = filtered_df.filter(col(\"clean_name\") == \"uber technologies \")\n",
    "\n",
    "# Run the UDF on the individual 'content' column\n",
    "# This allows Spark to batch them (e.g., 100 articles at a time) automatically\n",
    "analyzed_df = uber_df.withColumn(\n",
    "    \"sentiment_score\", \n",
    "    calculate_contextual_sentiment(col(\"content\"), col(\"clean_name\"))\n",
    ")\n",
    "\n",
    "display(analyzed_df.select(\"clean_name\", \"title\", \"sentiment_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3895c2aa-d361-486e-a8e7-a8e51280ee7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "analyzed_df = uber.withColumn(\n",
    "    \"relevant_sentences\", \n",
    "    calculate_contextual_sentiment(uber[\"article_contents\"], uber[\"clean_name\"])\n",
    ")\n",
    "# analyzed_df.createOrReplaceTempView(\"stock_prediction.default.articles_with_sentiment_view\")\n",
    "# analyzed_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"stock_prediction.default.articles_with_sentences\")\n",
    "display(analyzed_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5011200767627563,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sentiment_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
