{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012c31be-6cff-4254-8c1b-f1e2d4e5af6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch pandas nltk\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b92fc55-09be-41dd-b21c-d3236bae4445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\", return_all_scores=True, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41c3dcf1-8c45-47d6-bade-4e7edc63163e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab', download_dir='/tmp/nltk_data', quiet=True)\n",
    "sentence_tokenization_pipe = sent_tokenize\n",
    "text = \"\"\"\n",
    "Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n",
    "\"\"\"\n",
    "result = sentence_tokenization_pipe(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df80cb2-ee57-49d4-be5e-9eaab38ae166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from pyspark.sql.functions import pandas_udf, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@pandas_udf('double')\n",
    "def calculate_contextual_sentiment(\n",
    "    text_series: pd.Series, \n",
    "    company_names: pd.Series\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the aggregate sentiment score for an article based ONLY on sentences \n",
    "    that mention the corresponding company name.\n",
    "    \"\"\"\n",
    "    \n",
    "    final_scores = []\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    # 2. Iterate through the corresponding article content and company name pairs\n",
    "    # zip() ensures we process the rows in the batch together.\n",
    "    for article_content, company_name in zip(text_series, company_names):\n",
    "        \n",
    "        if not article_content:\n",
    "            final_scores.append(0.0) # Assign neutral score if no content\n",
    "            continue\n",
    "\n",
    "        # 3. Split the article into sentences (using the initialized function)\n",
    "        sentences = sentence_tokenization_pipe(article_content)\n",
    "        lower_company = company_name.lower()\n",
    "        \n",
    "        # 4. Filter the sentences based on the company name\n",
    "        relevant_sentences.append (\n",
    "            s for s in sentences\n",
    "            if lower_company in s.lower()\n",
    "        )\n",
    "        \n",
    "        # 5. Feed relevant sentences to FinBERT for sentiment score\n",
    "        if not relevant_sentences:\n",
    "            final_scores.append(0.0) # Assign neutral score if no relevant sentences\n",
    "            continue\n",
    "\n",
    "        # Run inference on the list of relevant sentences\n",
    "        # The model automatically processes the batch of sentences efficiently.\n",
    "        results = sentiment_pipe(relevant_sentences, truncation=True, max_length=512)\n",
    "        \n",
    "        # 6. Calculate the aggregate sentiment score\n",
    "        sentence_scores = []\n",
    "        for res in results:\n",
    "            # Extract scores for Positive and Negative labels\n",
    "            pos = next(item['score'] for item in res if item['label'] == 'positive')\n",
    "            neg = next(item['score'] for item in res if item['label'] == 'negative')\n",
    "            \n",
    "            # Compound score (-1 to 1)\n",
    "            sentence_scores.append(pos - neg)\n",
    "        \n",
    "        # Calculate the final article score as the mean of all relevant sentence scores\n",
    "        if sentence_scores:\n",
    "            final_scores.append(np.mean(sentence_scores))\n",
    "        else:\n",
    "            final_scores.append(0.0)\n",
    "\n",
    "    # 7. Return the final scores as a Pandas Series\n",
    "    return pd.Series(final_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4fcdce-2b9e-47eb-8e22-30f7640a959f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "companies_df = spark.read.table(\"stock_prediction.default.companies\")\n",
    "\n",
    "# Removes punctuation\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"name\"), '[^a-zA-Z0-9\\\\s]', '')\n",
    ")\n",
    "\n",
    "regex_pattern = r\"\\b(Inc|Corporation|Incorporated|Corp|Ltd|Co)\\b\"\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\", \n",
    "    sf.regexp_replace(sf.col(\"clean_name\"), regex_pattern, '')\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumn(\n",
    "    \"clean_name\",\n",
    "    sf.lower(sf.col(\"clean_name\"))\n",
    ")\n",
    "\n",
    "companies_df = companies_df.withColumnRenamed(\"id\", \"company_id\")\n",
    "\n",
    "display(companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92b2e83-335b-4ac5-b71b-bd54978cae81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "articles_df = spark.read.table(\"stock_prediction.default.articles\")\n",
    "\n",
    "# Cross join articles with companies to check for company mentions in article content\n",
    "joined_df = articles_df.crossJoin(companies_df)\n",
    "\n",
    "# Filter where clean_name is contained in content_cleaned (case-insensitive)\n",
    "filtered_df = joined_df.filter(\n",
    "    expr(\"lower(content_cleaned) LIKE concat('%', clean_name, '%')\")\n",
    ")\n",
    "\n",
    "# Group by company and collect articles mentioning each company\n",
    "grouped_df = filtered_df.groupBy(\"clean_name\").agg(\n",
    "    sf.collect_list(\"id\").alias(\"article_ids\"),\n",
    "    sf.collect_list(\"title\").alias(\"article_titles\"),\n",
    "    sf.collect_list(\"content\").alias(\"article_contents\")\n",
    ")\n",
    "\n",
    "display(grouped_df.select(\"clean_name\", \"article_ids\", \"article_titles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b881b174-431d-4b6d-a5ef-c24302cffa49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2. Apply the UDF\n",
    "# This adds a new column 'sentiment_score' to your DataFrame\n",
    "analyzed_df = grouped_df.withColumn(\n",
    "    \"sentiment_score\", \n",
    "    calculate_contextual_sentiment(grouped_df[\"article_contents\"], grouped_df[\"clean_name\"])\n",
    ")\n",
    "\n",
    "# analyzed_df.show()\n",
    "# analyzed_df.createOrReplaceTempView(\"stock_prediction.default.articles_with_sentiment_view\")\n",
    "# analyzed_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"stock_prediction.default.articles_with_sentences\")\n",
    "\n",
    "# Verify\n",
    "display(analyzed_df.limit(2))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-28 16_12_25",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
